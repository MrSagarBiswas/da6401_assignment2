# -*- coding: utf-8 -*-
"""ResNet50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15YiPdREgpM6VlHL9V7qUn9cYGR60jv7S
"""

pip install pytorch_lightning

import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import transforms, datasets
from torchvision.models import resnet50, ResNet50_Weights
import pytorch_lightning as pl
from pytorch_lightning.loggers import WandbLogger
from sklearn.model_selection import StratifiedShuffleSplit

from google.colab import drive
drive.mount('/content/drive')

# -------------------------
# 1. Data and Transforms
# -------------------------
DATA_DIR = "/content/drive/MyDrive/nature_12K/inaturalist_12K"
IMG_SIZE = 224      # Standard size for ResNet
BATCH_SIZE = 32
NUM_CLASSES = 10

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Standard ImageNet normalization
mean = [0.485, 0.456, 0.406]
std  = [0.229, 0.224, 0.225]

train_transforms = transforms.Compose([
    transforms.Resize(IMG_SIZE),
    transforms.CenterCrop(IMG_SIZE),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

val_transforms = transforms.Compose([
    transforms.Resize(IMG_SIZE),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

# Load full train folder (no transform) to get labels for stratification
full_train = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=None)
labels = [label for _, label in full_train.samples]

# Stratified split: 80% train, 20% validation
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, val_idx = next(sss.split(np.zeros(len(labels)), labels))

# Create Subsets with appropriate transforms
train_dataset = Subset(
    datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=train_transforms),
    train_idx
)
val_dataset = Subset(
    datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=val_transforms),
    val_idx
)

# Test dataset
test_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'test'), transform=val_transforms)

# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

# -------------------------
# 2. Model Setup
# -------------------------
# Load pretrained ResNet50 using new weights API
model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
in_feats = model.fc.in_features
model.fc = nn.Linear(in_feats, NUM_CLASSES)
model = model.to(device)

# -------------------------
# 3. Freezing Strategies
# -------------------------
def freeze_all_but_fc(model):
    for name, param in model.named_parameters():
        if 'fc' not in name:
            param.requires_grad = False

def freeze_up_to_layer(model, layer_name):
    freeze = True
    for name, param in model.named_parameters():
        if freeze:
            param.requires_grad = False
        if layer_name in name:
            freeze = False

# Choose a strategy:
freeze_all_but_fc(model)
# freeze_up_to_layer(model, 'layer3')
# (or comment out both to fine-tune entire model)

# -------------------------
# 4. Lightning Module
# -------------------------
class FineTuneModule(pl.LightningModule):
    def __init__(self, model, lr=1e-4):
        super().__init__()
        self.model = model
        self.lr = lr
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        acc = (logits.argmax(dim=1) == y).float().mean()
        self.log('train_loss_resnet50', loss, prog_bar=True, on_epoch=True)
        self.log('train_acc_resnet50', acc, prog_bar=True, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        acc = (logits.argmax(dim=1) == y).float().mean()
        self.log('val_loss_resnet50', loss, prog_bar=True, on_epoch=True)
        self.log('val_acc_resnet50', acc, prog_bar=True, on_epoch=True)

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        acc = (logits.argmax(dim=1) == y).float().mean()
        self.log('test_loss_resnet50', loss, prog_bar=True)
        self.log('test_acc_resnet50', acc, prog_bar=True)

    def configure_optimizers(self):
        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)
        return optimizer

# -------------------------
# Evaluation
# -------------------------
def evaluate_model(model, dataloader, device):
    model.eval()
    total, correct = 0, 0
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            preds = model(x).argmax(dim=1)
            correct += (preds == y).sum().item()
            total += y.size(0)
    return correct / total

# -------------------------
# 5. Training, Logging & Printing
# -------------------------
if __name__ == '__main__':
    wandb_logger = WandbLogger(project="CNN_inaturalist_12K", name="fine_tune_resnet50")
    module = FineTuneModule(model, lr=1e-4)
    trainer = pl.Trainer(
        max_epochs=10,
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        precision=16 if torch.cuda.is_available() else 32,
        logger=wandb_logger,
        log_every_n_steps=10
    )
    # Train and validate
    trainer.fit(module, train_loader, val_loader)

    # Test evaluation: logs test_acc and test_loss to wandb
    trainer.test(module, dataloaders=test_loader)

    # Manual evaluation and printing
    train_acc = evaluate_model(model, train_loader, device)
    val_acc   = evaluate_model(model, val_loader, device)
    test_acc  = evaluate_model(model, test_loader, device)
    print(f"Train Accuracy: {train_acc*100:.2f}%")
    print(f"Validation Accuracy: {val_acc*100:.2f}%")
    print(f"Test Accuracy: {test_acc*100:.2f}%")

    # Save model weights
    torch.save(model.state_dict(), 'fine_tuned_resnet50.pth')

